{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 작성일 : 24.8.17 \n",
    "import os\n",
    "\n",
    "if 'original_dir' not in globals() :\n",
    "    original_dir = os.getcwd()\n",
    "    original_dir = os.path.dirname(os.path.dirname((os.path.dirname(original_dir))))\n",
    "    os.chdir(original_dir)\n",
    "from setup.default import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(original_dir+'/data/preproc/main/feature_lab_v1/menu_w_soldout_w_soldout_ratio.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# 결측치 처리\n",
    "df.loc[:,'course_kcal'] = df['course_kcal'].fillna(df['course_kcal'].mean().astype(int))\n",
    "df.loc[:,'course_protein'] = df['course_protein'].fillna(df['course_protein'].mean().astype(int))\n",
    "df.loc[:, 'course_na'] = df['course_na'].apply(lambda x: int(x) if pd.notna(x) else np.nan)\n",
    "df.loc[:,'course_na'] = df['course_na'].fillna(df['course_na'].mean().astype(int))\n",
    "df.loc[:,'soldout_ratio'] = df['soldout_ratio'].fillna(df['soldout_ratio'].mean().astype(int))\n",
    "df.loc[:,'soldout_ratio_c'] = df['soldout_ratio_c'].fillna(df['soldout_ratio_c'].mean().astype(int))\n",
    "\n",
    "# 루트 화\n",
    "df['sim_menu'] = np.sqrt(df['sim_menu'])\n",
    "df['sim_menu_c'] = np.sqrt(df['sim_menu_c'])\n",
    "\n",
    "# # StandardScaler를 사용하여 표준 정규 분포로 변환\n",
    "columns_to_scale = ['course_kcal', 'course_protein', 'course_na']\n",
    "scaler = StandardScaler()\n",
    "df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n",
    "\n",
    "\n",
    "# 특징과 타겟 변수 설정\n",
    "df['day'] = pd.to_datetime(df['day'])\n",
    "\n",
    "test_data = df[(df['day'] >= '2024-04-01')]\n",
    "train_data = df[(df['day'] < '2024-04-01')]\n",
    "\n",
    "# 피처와 타겟 분리\n",
    "X_train = train_data.drop(columns=['post_no','day','meal_time','is_soldout', 'course', 'soldout','soldout_time','first_menu_unpreproc',\\\n",
    "                                   'first_menu','menu_no','only_menu_2','only_menu_2_soldout','course_no_c','only_menu_2_c','only_menu_2_soldout_c'])\n",
    "y_train = train_data['is_soldout'].astype(int)\n",
    "\n",
    "# y_train이 1인 샘플의 50%를 무작위로 샘플링하여 추가 (1.5배로 증강)\n",
    "X_train_1 = X_train[y_train == 1]\n",
    "y_train_1 = y_train[y_train == 1]\n",
    "\n",
    "X_train_1_resampled, y_train_1_resampled = resample(X_train_1, y_train_1,\n",
    "                                                     replace=True,\n",
    "                                                     n_samples=int(len(X_train_1) * 0.8),\n",
    "                                                     random_state=None)\n",
    "\n",
    "# 기존 X_train, y_train과 결합\n",
    "X_train_resampled = pd.concat([X_train, X_train_1_resampled])\n",
    "y_train_resampled = pd.concat([y_train, y_train_1_resampled])\n",
    "\n",
    "X_test = test_data.drop(columns=['post_no','day','meal_time','is_soldout', 'course', 'soldout','soldout_time','first_menu_unpreproc',\\\n",
    "                                   'first_menu','menu_no','only_menu_2','only_menu_2_soldout','course_no_c','only_menu_2_c','only_menu_2_soldout_c'])\n",
    "y_test = test_data['is_soldout'].astype(int)\n",
    "\n",
    "# 범주형 변수 인코딩 및 스케일링\n",
    "numeric_features = ['course_kcal', 'course_protein', 'course_na','sim_menu','sim_menu_c','soldout_ratio','soldout_ratio_c']\n",
    "categorical_features = ['day_of_week', 'course_no']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.53125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.68      0.65        41\n",
      "           1       0.32      0.26      0.29        23\n",
      "\n",
      "    accuracy                           0.53        64\n",
      "   macro avg       0.47      0.47      0.47        64\n",
      "weighted avg       0.51      0.53      0.52        64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 로지스틱 회귀 모델 파이프라인\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "# 모델 훈련\n",
    "model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# 예측\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 결과 평가\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.640625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.76      0.73        41\n",
      "           1       0.50      0.43      0.47        23\n",
      "\n",
      "    accuracy                           0.64        64\n",
      "   macro avg       0.60      0.60      0.60        64\n",
      "weighted avg       0.63      0.64      0.63        64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier  # 다른 모델을 사용하려면 해당 클래스를 임포트\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ]\n",
    ")\n",
    "# 로지스틱 회귀 대신 랜덤 포레스트를 사용한 파이프라인\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# 모델 훈련 및 평가\n",
    "model.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 결과 평가\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.59375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.73      0.70        41\n",
      "           1       0.42      0.35      0.38        23\n",
      "\n",
      "    accuracy                           0.59        64\n",
      "   macro avg       0.54      0.54      0.54        64\n",
      "weighted avg       0.58      0.59      0.58        64\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/al01989093/anaconda3/lib/python3.11/site-packages/xgboost/core.py:158: UserWarning: [14:53:19] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# XGBoost 모델 파이프라인\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', XGBClassifier(n_estimators=100, random_state=42, use_label_encoder=False, eval_metric='logloss'))\n",
    "])\n",
    "\n",
    "# 모델 훈련\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 예측\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 결과 평가\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 181, number of negative: 207\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000143 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 302\n",
      "[LightGBM] [Info] Number of data points in the train set: 388, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.466495 -> initscore=-0.134222\n",
      "[LightGBM] [Info] Start training from score -0.134222\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Accuracy: 0.515625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.63      0.63        41\n",
      "           1       0.32      0.30      0.31        23\n",
      "\n",
      "    accuracy                           0.52        64\n",
      "   macro avg       0.47      0.47      0.47        64\n",
      "weighted avg       0.51      0.52      0.51        64\n",
      "\n",
      "# of soldout :  22\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='lightgbm')\n",
    "\n",
    "# LightGBM을 사용한 파이프라인\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LGBMClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# 모델 훈련 및 평가\n",
    "model.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 결과 평가\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f'# of soldout : ', y_pred.sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그래디언트 부스팅 머신 (Gradient Boosting Machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.59375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.66      0.68        41\n",
      "           1       0.44      0.48      0.46        23\n",
      "\n",
      "    accuracy                           0.59        64\n",
      "   macro avg       0.57      0.57      0.57        64\n",
      "weighted avg       0.60      0.59      0.60        64\n",
      "\n",
      "# of soldout :  25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# 그래디언트 부스팅을 사용한 파이프라인\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GradientBoostingClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# 모델 훈련 및 평가\n",
    "model.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 결과 평가\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f'# of soldout : ', y_pred.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.640625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.78      0.74        41\n",
      "           1       0.50      0.39      0.44        23\n",
      "\n",
      "    accuracy                           0.64        64\n",
      "   macro avg       0.60      0.59      0.59        64\n",
      "weighted avg       0.63      0.64      0.63        64\n",
      "\n",
      "# of soldout :  18\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# CatBoost를 사용한 파이프라인\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', CatBoostClassifier(iterations=100, random_state=42, verbose=0))\n",
    "])\n",
    "\n",
    "# 모델 훈련 및 평가\n",
    "model.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 결과 평가\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f'# of soldout : ', y_pred.sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# # 예시 데이터 프레임 생성\n",
    "# df = pd.read_csv(original_dir+'/data/preproc/main/feature_lab_v1/menu_w_soldout_w_soldout_ratio.csv')\n",
    "\n",
    "# # 결측치 처리\n",
    "# df.loc[:,'course_kcal'] = df['course_kcal'].fillna(df['course_kcal'].mean().astype(int))\n",
    "# df.loc[:,'course_protein'] = df['course_protein'].fillna(df['course_protein'].mean().astype(int))\n",
    "# df.loc[:, 'course_na'] = df['course_na'].apply(lambda x: int(x) if pd.notna(x) else np.nan)\n",
    "# df.loc[:,'course_na'] = df['course_na'].fillna(df['course_na'].mean().astype(int))\n",
    "# # df.loc[:,'soldout_ratio'] = df['soldout_ratio'].fillna(df['soldout_ratio'].mean().astype(int))\n",
    "# # df.loc[:,'soldout_ratio_c'] = df['soldout_ratio_c'].fillna(df['soldout_ratio_c'].mean().astype(int))\n",
    "\n",
    "# # 루트 화\n",
    "# df['sim_menu'] = np.sqrt(df['sim_menu'])\n",
    "# df['sim_menu_c'] = np.sqrt(df['sim_menu_c'])\n",
    "\n",
    "# # # StandardScaler를 사용하여 표준 정규 분포로 변환\n",
    "# columns_to_scale = ['course_kcal', 'course_protein', 'course_na']\n",
    "# scaler = StandardScaler()\n",
    "# df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n",
    "\n",
    "\n",
    "# # 특징과 타겟 변수 설정\n",
    "# df['day'] = pd.to_datetime(df['day'])\n",
    "\n",
    "# test_data = df[(df['day'] >= '2024-04-01')]\n",
    "# train_data = df[(df['day'] < '2024-04-01')]\n",
    "\n",
    "# # 피처와 타겟 분리\n",
    "# X_train = train_data.drop(columns=['post_no','day','meal_time','is_soldout', 'course', 'soldout','soldout_time','first_menu_unpreproc',\\\n",
    "#                                    'first_menu','menu_no','only_menu_2','only_menu_2_soldout','course_no_c','only_menu_2_c','only_menu_2_soldout_c'])\n",
    "# y_train = train_data['is_soldout'].astype(int)\n",
    "\n",
    "# # y_train이 1인 샘플의 50%를 무작위로 샘플링하여 추가 (1.5배로 증강)\n",
    "# X_train_1 = X_train[y_train == 1]\n",
    "# y_train_1 = y_train[y_train == 1]\n",
    "\n",
    "# X_train_1_resampled, y_train_1_resampled = resample(X_train_1, y_train_1,\n",
    "#                                                      replace=True,\n",
    "#                                                      n_samples=int(len(X_train_1) * 0.8),\n",
    "#                                                      random_state=None)\n",
    "\n",
    "# # 기존 X_train, y_train과 결합\n",
    "# X_train_resampled = pd.concat([X_train, X_train_1_resampled])\n",
    "# y_train_resampled = pd.concat([y_train, y_train_1_resampled])\n",
    "\n",
    "# X_test = test_data.drop(columns=['post_no','day','meal_time','is_soldout', 'course', 'soldout','soldout_time','first_menu_unpreproc',\\\n",
    "#                                    'first_menu','menu_no','only_menu_2','only_menu_2_soldout','course_no_c','only_menu_2_c','only_menu_2_soldout_c'])\n",
    "# y_test = test_data['is_soldout'].astype(int)\n",
    "\n",
    "# # 범주형 변수 인코딩 및 스케일링\n",
    "# numeric_features = ['course_kcal', 'course_protein', 'course_na','sim_menu','sim_menu_c','soldout_ratio','soldout_ratio_c']\n",
    "# categorical_features = ['day_of_week', 'course_no']\n",
    "\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', StandardScaler(), numeric_features),\n",
    "#         ('cat', OneHotEncoder(), categorical_features)\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # XGBoost 모델 파이프라인\n",
    "# model = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('classifier', XGBClassifier(n_estimators=100, random_state=42, use_label_encoder=False, eval_metric='logloss'))\n",
    "# ])\n",
    "\n",
    "# # 모델 훈련\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # 예측\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# # 결과 평가\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f'Accuracy: {accuracy}')\n",
    "# print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
